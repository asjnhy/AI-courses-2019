{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEAM4_Poly_Parallel_Conv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asjnhy/AI-courses-2019/blob/master/MNIST-Polynomial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MQpD1XmZ6f6q"
      },
      "source": [
        "# Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VfRhw-6M3R4x",
        "outputId": "d249389f-05d7-4ef4-a9ef-b71b29c0968c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x7FW2Rp2T6f",
        "colab_type": "code",
        "outputId": "75facc9e-05b5-46f6-da00-44d8d2d8fbd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!pip install cupy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cupy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/89/99f980706c61e6b96a579a81dea3eb68c22df1b526bf357673be5e18fe31/cupy-7.0.0.tar.gz (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy) (1.17.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy) (1.12.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy) (0.4)\n",
            "Building wheels for collected packages: cupy\n",
            "  Building wheel for cupy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cupy: filename=cupy-7.0.0-cp36-cp36m-linux_x86_64.whl size=28356280 sha256=ad54cc16377026383458b559e3e3f6fc1a8f17cb1a1702b0d073925a85660897\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/33/37/f412224f7550a11ee18a9f41f7aff28df380bfa994f7280ab3\n",
            "Successfully built cupy\n",
            "Installing collected packages: cupy\n",
            "Successfully installed cupy-7.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lf2kcWr2xfS5"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JX3uv-d9whWt",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import struct\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyadthozxkvE"
      },
      "source": [
        "# Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LUQO7TzZxVH4",
        "colab": {}
      },
      "source": [
        "class BinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, batch_size=16, max_iter=50, learning_rate=0.01, random_state=1, C=100):\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iter = max_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.random_state = random_state\n",
        "        self.C = C\n",
        "        self.rgen = np.random.RandomState(self.random_state)\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        # Exception Handling\n",
        "        if self.C < 0:\n",
        "            raise ValueError(\"The C value of %r must be positive\" % self.C)\n",
        "        if ((self.learning_rate < 0) or (self.learning_rate > 1)):\n",
        "            raise ValueError(\"The learning_rate value of %r is invalid.\" % self.learning_rate,\n",
        "                             \"Set the learning_rate value between 0.0 and 1.0.\")        \n",
        "            \n",
        "        n_batches = math.ceil(len(X) / self.batch_size)\n",
        "        rest_batch_size = X.shape[0] - (n_batches-1) * self.batch_size\n",
        "        \n",
        "        self.w_ = cp.array(self.rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]))\n",
        "        self.b_ = 0.\n",
        "        \n",
        "        for epoch in range(self.max_iter):\n",
        "            X, y = self.shuffle(X, y)\n",
        "            \n",
        "            for j in range(n_batches - 1):\n",
        "                self.calculateGradientAndUpdate(X, y, batch_size = self.batch_size, n_batch = j)\n",
        "            self.calculateGradientAndUpdate(X, y, batch_size = rest_batch_size, n_batch = j)\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    def hypothesis(self, X):\n",
        "        return cp.dot(cp.array(X), self.w_) + self.b_\n",
        "    \n",
        "    def shuffle(self, X, y):\n",
        "        shuffle_index = np.arange(X.shape[0])\n",
        "        np.random.shuffle(shuffle_index)\n",
        "        return X[shuffle_index], y[shuffle_index]\n",
        "    \n",
        "    def calculateGradientAndUpdate(self, X, y, batch_size, n_batch):\n",
        "        X_mini = X[n_batch*batch_size : (n_batch+1)*batch_size]\n",
        "        y_mini = y[n_batch*batch_size : (n_batch+1)*batch_size]\n",
        "        X_mini = cp.array(X_mini)\n",
        "        y_mini = cp.array(y_mini)\n",
        "\n",
        "        grad_w = cp.zeros(X.shape[1])\n",
        "        grad_b = 0\n",
        "        mask = cp.less_equal(cp.multiply(y_mini, self.hypothesis(X_mini)), 1)\n",
        "        \n",
        "        Xy = cp.multiply(X_mini.T, y_mini)\n",
        "        masked_Xy = cp.multiply(Xy, mask)\n",
        "        grad_w = cp.sum(-masked_Xy, axis=1)\n",
        "        grad_w /= batch_size\n",
        "        grad_w += self.w_/self.C\n",
        "        self.w_ -= self.learning_rate * grad_w\n",
        "        \n",
        "        masked_y = cp.multiply(y_mini, mask)\n",
        "        grad_b = cp.sum(-masked_y, axis=0)\n",
        "        grad_b = grad_b / batch_size\n",
        "        self.b_ -= self.learning_rate * grad_b\n",
        "        return grad_w, grad_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_yQihMyWxso8"
      },
      "source": [
        "# Multiclass Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J8jHzGocxwLm",
        "colab": {}
      },
      "source": [
        "class MulticlassClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, batch_size=16, max_iter=50, learning_rate=0.01, random_state=1, C=100):\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iter = max_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.random_state = random_state\n",
        "        self.C = C\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        self.labels = np.unique(y) # 0 ~ 9\n",
        "        # self.labels = cp.unique(y) # 0 ~ 9\n",
        "        self.outputs_ = []\n",
        "        for label in range(len(self.labels)):\n",
        "            # y_binary = cp.where(cp.array(y == label), 1, -1)\n",
        "            y_binary = np.where(np.array(y==label),1,-1)\n",
        "            b_c = BinaryClassifier(self.batch_size, self.max_iter, \n",
        "                                   self.learning_rate, self.random_state, self.C)\n",
        "            b_c.fit(X, y_binary)\n",
        "            self.outputs_.append(b_c)\n",
        "        return self\n",
        "        \n",
        "    def predict(self, X):\n",
        "        prediction = []\n",
        "        for o in self.outputs_:\n",
        "            prediction.append(o.hypothesis(X))\n",
        "        return self.labels[np.argmax(prediction, axis=0)]\n",
        "        # return self.labels[cp.argmax(prediction, axis=0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p4zskV6XyTNX"
      },
      "source": [
        "# MNIST Read Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y_cA1d2FxwhE",
        "colab": {}
      },
      "source": [
        "def read(images, labels):\n",
        "    with open(labels, 'rb') as lbpath:\n",
        "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
        "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
        "\n",
        "    with open(images, 'rb') as imgpath:\n",
        "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
        "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def read_no_label(images):\n",
        "    with open(images, 'rb') as imgpath:\n",
        "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
        "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(60000, 784)\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NM_gcg_X6u4u"
      },
      "source": [
        "# Read MNIST & Split for Valiation (80k)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "viA-V_NGySjf",
        "outputId": "dc46abca-9eaf-48d2-8a05-2edb39d6358f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 경로 수정하세요 !\n",
        "X, y = read(os.getcwd() + '/drive/My Drive/Colab Notebooks/data/newtrain-images-idx3-ubyte', \n",
        "            os.getcwd() + '/drive/My Drive/Colab Notebooks/data/newtrain-labels-idx1-ubyte')\n",
        "# X_test_no_label = read_no_label(os.getcwd()+'/data/testall-images-idx3-ubyte')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BYKNh4VglWOt"
      },
      "source": [
        "# Preprocessing : Convolutional Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZGTEGyFTmVuk"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fX4LRWR3lb3i",
        "colab": {}
      },
      "source": [
        "def zero_padding(img, n=1):\n",
        "\n",
        "    m = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    h = img.shape[2]\n",
        "    \n",
        "    padded_img = np.ones((m, w + 2 * n, h + 2 * n))\n",
        "    \n",
        "    padded_img[:, n : padded_img.shape[1] - n, n : padded_img.shape[2] - n] = img\n",
        "    \n",
        "    return padded_img\n",
        "    \n",
        "def horizontal_filter(img):\n",
        "    h_filter = np.array([\n",
        "        [ 0, 0, 0],\n",
        "        [ 0, 1, 0],\n",
        "        [-1, 0, 0]\n",
        "    ])\n",
        "    h_filter2 = np.array([\n",
        "        [-1, -2, -1],\n",
        "        [ 0,  0,  0],\n",
        "        [ 1,  2,  1]\n",
        "    ])\n",
        "    h_filter.reshape(1,3,3)\n",
        "    h_filter2.reshape(1,3,3)\n",
        "    m = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    h = img.shape[2]\n",
        "    horizontal_grad = np.zeros((m, w - 2, h - 2))\n",
        "    horizontal_grad2 = np.zeros((m, w - 2, h - 2))\n",
        "\n",
        "    for i in range(1, w - 1):\n",
        "        for j in range(1, h - 1):\n",
        "            images_slice = img[:, i - 1 : i + 2, j - 1 : j + 2]\n",
        "            horizontal_grad[:, i - 1, j - 1] = np.sum(np.multiply(images_slice, h_filter), axis=(1, 2))\n",
        "            horizontal_grad2[:, i - 1, j - 1] = np.sum(np.multiply(images_slice, h_filter2), axis=(1, 2))\n",
        "            \n",
        "    return horizontal_grad, horizontal_grad2\n",
        "    \n",
        "def MaxPooling(img):\n",
        "\n",
        "    m = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    h = img.shape[2]\n",
        "    \n",
        "    img = zero_padding(img, 1)\n",
        "    img2, img3 = horizontal_filter(img)\n",
        "    \n",
        "    pooling_grad = np.zeros((m, w//2, h//2))\n",
        "    pooling_grad2 = np.zeros((m, w//2, h//2))\n",
        "\n",
        "    for i in range(0, w//2):\n",
        "        for j in range(0, h//2):\n",
        "            pooling_grad[: , i , j ] = np.max(img2[: , 2*i : 2*i + 2, 2*j : 2*j + 2])\n",
        "            pooling_grad2[: , i , j ] = np.max(img3[: , 2*i : 2*i + 2, 2*j : 2*j + 2])\n",
        "\n",
        "    pooling_grad = pooling_grad.reshape(m,-1)\n",
        "    pooling_grad2 = pooling_grad.reshape(m,-1)    \n",
        "            \n",
        "    return pooling_grad, pooling_grad2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KaCiYfwUn3N",
        "colab_type": "text"
      },
      "source": [
        "## Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Z27LpNegNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_reshaped = X_train.reshape(X_train.shape[0], 28, 28)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], 28, 28)\n",
        "\n",
        "X_train_conv1, X_train_conv2 = MaxPooling(X_train_reshaped)\n",
        "X_test_conv1, X_test_conv2 = MaxPooling(X_test_reshaped)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdJUrZJEUsaj",
        "colab_type": "text"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvVMuMDx43VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.93)\n",
        "\n",
        "X_train_pca = pca.fit_transform(X_train) \n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s8SC4dGUuHQ",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6sF16Gu45CM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True, order='F')\n",
        "X_train_pca_poly = poly.fit_transform(X_train_pca)\n",
        "X_test_pca_poly = poly.transform(X_test_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC9ePziJUwr0",
        "colab_type": "text"
      },
      "source": [
        "# Check shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csxZxbmKJaMd",
        "colab_type": "code",
        "outputId": "82c1a6f2-c49b-45de-a988-d47548bbd155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"X_train : \", X_train.shape)\n",
        "print(\"X_train_conv1 : \", X_train_conv1.shape)\n",
        "print(\"X_train_conv2 : \", X_train_conv2.shape)\n",
        "print(\"X_train_pca : \", X_train_pca.shape)\n",
        "print(\"X_train_pca_poly : \", X_train_pca_poly.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train :  (56000, 784)\n",
            "X_train_conv1 :  (56000, 196)\n",
            "X_train_conv2 :  (56000, 196)\n",
            "X_train_pca :  (56000, 119)\n",
            "X_train_pca_poly :  (56000, 7260)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEMMsVmFUze0",
        "colab_type": "text"
      },
      "source": [
        "# Hstack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZE7srWPKXNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_pca_poly_conv12 = np.hstack((X_train_pca_poly,\n",
        "                                     X_train_conv1,\n",
        "                                     X_train_conv2))\n",
        "\n",
        "X_test_pca_poly_conv12 = np.hstack((X_test_pca_poly,\n",
        "                                    X_test_conv1,\n",
        "                                    X_test_conv2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWyDAyZSU2hF",
        "colab_type": "code",
        "outputId": "715c5571-e071-443b-b8f1-578aef92c49e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_pca_poly_conv12.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56000, 7652)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXpWTdClVZW-",
        "colab_type": "text"
      },
      "source": [
        "# Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZmWwwBL48UK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MC=MulticlassClassifier(max_iter=5, batch_size=256, learning_rate=0.01, C=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1edbSQqG4-OG",
        "colab_type": "code",
        "outputId": "2175ce2e-ce10-428b-a217-943b3a51a62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Start Time : \", datetime.now())\n",
        "\n",
        "start = time.time()\n",
        "MC.fit(X_train_pca_poly_conv12, y_train)\n",
        "\n",
        "y_pred = MC.predict(X_test_pca_poly_conv12)\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"End Time : \", datetime.now())\n",
        "print(\"Training Time : \", time.time() - start)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Time :  2019-12-14 17:07:18.581149\n",
            "End Time :  2019-12-14 17:10:44.478631\n",
            "Training Time :  205.89781498908997\n",
            "0.9747083333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2l6Ni5JVAxe",
        "colab_type": "text"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QE-S9VnAfWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# param_grid = [{\n",
        "#     'C' : [0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
        "#     'learning_rate' : [0.1, 0.01, 0.001],\n",
        "#     'batch_size' : [32],\n",
        "#     'max_iter' : [5]\n",
        "# }]\n",
        "\n",
        "# grid_search = GridSearchCV(MulticlassClassifier(), \n",
        "#                             param_grid=param_grid, \n",
        "#                             cv=10, scoring='accuracy',\n",
        "#                             n_jobs=-1)\n",
        "\n",
        "# grid_search.fit(X_train_pca_poly_conv12, y_train)\n",
        "# print(grid_search.best_params_)\n",
        "# print(grid_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}